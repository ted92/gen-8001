\section{Introduction}
%todo: talk about main difficulties in open research data in our field
%todo: IDENTIFY these difficulties
Before the advent of scientific journals,
science was more oriented towards a closed approach, so that only a restricted number
of people could have access to it.
Many scientists, including Galileo, Newton, Kepler, and Hooke, made their discoveries into
something they could profit, and often papers were even encoded in anagrams.
Following this closed science logic, it was difficult
to identify scientific priority of research discoveries, like for instance the
debate on whom first discovered calculus between Leibniz and Newton.

In modern times, scientific research is moving towards openness,
and technology has solved scientific priority issue to a certain extent.
Many scientists are in favor of open science, for instance
Merton argues that knowledge-creation is more efficient if scientists work together, and
it is morally binding on a professional scientist\,\cite{merton1942science}.
However, there exists research data, journals, and papers which are not openly accessible.
In some cases research data is not available due to privacy concerns.
Research data collected by for-profit organizations is held in secrecy, for
safeguarding commercial interests.
Furthermore, pay-walled journals limit access of scientific publications.
All this hinders the propagation of open science.

%go from reproducibility to physics then cs and then machine learning, and stick with it!
Besides seeking knowledge, science needs also to constantly proof itself
to be right, hence it needs to be \emph{reproducible}. 
In sixteenth and seventeenth centuries, scientists such as Newton or Galileo could guarantee
reproducibility in physics by using mathematical formulas.
%
In the late twentieth century, in the field of \ac{cs},
\emph{pseudo code}\footnote{pseudo code is the logic or abstraction which explains algorithms.}
helped reproducibility.

In twenty-first century, \ac{ml} rapidly gained popularity\,\cite{mjolsness2001machine}.
\ac{ml} relies on new techniques based on big data analysis and statistical inference.
With \ac{ml} it is possible to train machines to solve particular tasks without being
given specific instructions.
%
\ac{ml} has drastically changed the way of doing research\,\cite{anderson2008end},
so that mathematical formulas and pseudo code are not
enough to guarantee reproducibility, while research data, source code,
and model's set up, became fundamental information in order to reconstruct the
same outcomes of an experiment.

In this manuscript we want to enhance the main challenges related to reproducibility
in \ac{ml}. More importance should be given to open source
code and open research data, in order to perform good science.
Excessively closed research data can lead to reproducibility issues,
especially when big data analysis is involved.

% you need to tune parameters
\section{Reproducibility Issue}
Scientific discoveries suffer of reproducibility due
to selective reporting, selective analysis, or insufficient specification to recreate the
expected results\,\cite{aarts2016reproducibility}. 
In \ac{ml}, these specifications include tuning parameters for statistical models, which are crucial
for replication. Furthermore, tuning parameters
is a very sensitive issue both in practical applications and in academic studies\,\cite{birattari2004problem}.

In addition to the amount of information that \ac{ml} research needs to be reproducible,
there is a large amount of published researches which cannot be
reproduced\,\cite{begley2012drug, begley2015reproducibility, prinz2011believe},
and some researchers are not willing to share code and data\,\cite{gundersen2018state}.
In a study conducted by Colleberg and Proebsting\,\cite{Collberg:2016:RCS:2897191.2812803},
even in open access journals, such as \ac{acm}, only $66\%$ of the experimental papers
were backed by code and only $32\%$ of those were easily reproducible.
% reproducibility with \ac{ai} is not guaranteed with paper, data and pseudo code, also all the settings to make the model work are needed. And following the study of Gundersen\,\cite{gundersen2018state}, conducted in other conferences, out of $400$ algorithms presented, only $6/%$ included the source code, $30/%$ included test data and $54/%$ included pseudo code.
%todo: continue here to add above? ^

%todo: CLAIM that reproducibility comes with selective reporting and analysis
In \ac{cs}, while developing algorithms, it suffices to share an abstraction of the code which
mathematically explain its logic (\emph{pseudo code}),
in order to enable reproducibility. In \ac{ai} and \ac{ml} it is a bit more complex. In order to
generate a \ac{ml} model, big amount of data is needed, 
this data can be manipulated, normalized, and maybe just a variable
change in the optimization function can trigger the outcome
of the experiment. Reproducibility in \ac{cs} and \ac{ml}
then becomes fragile, and because of this,
there is more need of data and code openness. However,
%todo: enhance the key point of this section: ehnance a search on research data and show the main challenges for sharing research data


%todo: insert this --> closed models or data can slow down the scientific process, hence the progress

%todo: check on google datasets search

\section{Conclusions}
% especially in \ac{cs} and \ac{ml} we believe that reproducibility in science is the key value that should enable data and source code to be shared. Data need to be shared if results want to be reproduced, hence confirm the scientific discovery
%todo: some data are sensitive and needs to be preserved and protected.

% todo: talk about privacy online and how open access can affect the resarch or reserachers in certain fileds
%todo: different open science schools\,\cite{Fecher2014}

%\newpage
% Bibliography
\bibliographystyle{acm}
\bibliography{bibliography}
